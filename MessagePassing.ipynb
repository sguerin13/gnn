{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "General Message Passing:\n",
    "- x_i is the node feature\n",
    "- e_ji is an optional edge feature\n",
    "- square is a diff perm invariant agg function\n",
    "- gamma and phi are differential fn, like MLP\n",
    "\n",
    "$$\\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right),$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyG provides the MessagePassing base class, which helps in creating such kinds of message passing graph neural networks by automatically taking care of message propagation. The user only has to define the functions  , $\\phi$, i.e. message(), and $\\gamma$, i.e. update(), as well as the aggregation scheme to use, i.e. aggr=\"add\", aggr=\"mean\" or aggr=\"max\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MessagePassing(aggr=\"add\", flow=\"source_to_target\", node_dim=-2): Defines the aggregation scheme to use (\"add\", \"mean\" or \"max\") and the flow direction of message passing (either \"source_to_target\" or \"target_to_source\"). Furthermore, the node_dim attribute indicates along which axis to propagate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MessagePassing.propagate(edge_index, size=None, **kwargs): The initial call to start propagating messages. Takes in the edge indices and all additional data which is needed to construct messages and to update node embeddings. Note that propagate() is not limited to exchanging messages in square adjacency matrices of shape [N, N] only, but can also exchange messages in general sparse assignment matrices, e.g., bipartite graphs, of shape [N, M] by passing size=(N, M) as an additional argument. If set to None, the assignment matrix is assumed to be a square matrix. For bipartite graphs with two independent sets of nodes and indices, and each set holding its own information, this split can be marked by passing the information as a tuple, e.g. x=(x_N, x_M)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MessagePassing.message(...): Constructs messages to node  in analogy to $\\phi$ for each edge $(j,i) \\in \\mathcal{E}$ if flow=\"source_to_target\" and $(i,j) \\in \\mathcal{E}$ if flow=\"target_to_source\". Can take any argument which was initially passed to propagate(). In addition, tensors passed to propagate() can be mapped to the respective nodes  and  by appending _i or _j to the variable name, e.g. x_i and x_j. Note that we generally refer to i as the central nodes that aggregates information, and refer to  j as the neighboring nodes, since this is the most common notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MessagePassing.update(aggr_out, ...): Updates node embeddings in analogy to $\\gamma$ for each node $i \\in \\mathcal{V}$. Takes in the output of aggregation as first argument and any argument which was initially passed to propagate()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the GCN Layer \n",
    "\n",
    "$$\\mathbf{x}_i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot \\left( \\mathbf{W}^{\\top} \\cdot \\mathbf{x}_j^{(k-1)} \\right) + \\mathbf{b},$$\n",
    "\n",
    "Before Message Passing:\n",
    "\n",
    "- Add self-loops to the adjacency matrix.\n",
    "\n",
    "- Linearly transform node feature matrix.\n",
    "\n",
    "- Compute normalization coefficients.\n",
    "\n",
    "During Message Passing: \n",
    "\n",
    "- Normalize node features in .\n",
    "\n",
    "- Sum up neighboring node features (\"add\" aggregation).\n",
    "\n",
    "Finally:\n",
    "- Apply a final bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Linear, Parameter, ReLU,  Sequential as Seq\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNLayer(MessagePassing):\n",
    "    # Initialize the class with the aggregration type\n",
    "    def __init__(self,aggr='add'):\n",
    "        self.lin = Linear\n",
    "    \n",
    "    def forward(self,x,edge_index):\n",
    "        '''\n",
    "        \n",
    "        Do the math inside the aggregation function Phi\n",
    "\n",
    "        Then propagate the 'message':\n",
    "            - propagate internally calls:\n",
    "                - message, aggregate, and update\n",
    "        \n",
    "        '''\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        '''define the message that is passed via propagation'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self,in_channels, out_channels): # Message Passing Initialization Component\n",
    "        super().__init__(aggr=\"add\") # initialize with aggregation mode of 'add', could also add flow, and node_dim args\n",
    "        self.lin = Linear(in_channels, out_channels)\n",
    "        self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters() # resets to initialization method: torch uses kaiming by default\n",
    "        self.bias.data.zero_() # zeros all values in the tensor\n",
    "    \n",
    "\n",
    "    ''' forward contains the propagate step, which'''\n",
    "    def forward(self, x, edge_index):\n",
    "        # x is NxIn_Channels\n",
    "        # edge_index = 2xE\n",
    "\n",
    "        # step 1: Add loop to adj matrix\n",
    "        edge_index = add_self_loops(edge_index,num_nodes=x.size(0))\n",
    "        \n",
    "        # step 2: Linearly Transform X\n",
    "        wTx = self.lin(x)\n",
    "\n",
    "        # step 3: compute normalization\n",
    "        row, col = edge_index # row: all i's in edge matrix, col: all j's in adjacency matrix\n",
    "        # uses scatter_add to count up the node degree at a given index value\n",
    "        # counts all the nodes with incoming edges to calculate the degree for all the j values\n",
    "        deg = degree(col, x.size(0))\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col] # access them by their indexes\n",
    "\n",
    "        # steps 4 and 5: normalize and sum node features\n",
    "        '''\n",
    "        We then call propagate(), which internally calls message(),\n",
    "        aggregate() and update(). We pass the node embeddings x and \n",
    "        the normalization coefficients norm as additional arguments \n",
    "        for message propagation.'''\n",
    "        out = self.propagate(edge_index, x=wTx, norm=norm)\n",
    "\n",
    "        # 6: add bias:\n",
    "        out = out + self.bias\n",
    "    \n",
    "    def message(self, x_j, norm):\n",
    "        '''\n",
    "        In the message() function, we need to normalize the neighboring\n",
    "         node features x_j by norm. Here, x_j denotes a lifted tensor, \n",
    "         which contains the source node features of each edge, i.e., the \n",
    "         neighbors of each node. Node features can be automatically lifted \n",
    "         by appending _i or _j to the variable name. In fact, any tensor \n",
    "         can be converted this way, as long as they hold source or \n",
    "         destination node features.\n",
    "        '''\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1,1)*x_j\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing an EdgeConv Layer ##\n",
    "\n",
    "$$\\mathbf{x}_i^{(k)} = \\max_{j \\in \\mathcal{N}(i)} h_{\\mathbf{\\Theta}} \\left( \\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)} - \\mathbf{x}_i^{(k-1)} \\right),$$\n",
    "\n",
    "where $h_{\\mathbf{\\Theta}}$ denotes an MLP. In analogy to the GCN layer, we can use the MessagePassing class to implement this layer, this time using the channelwise \"max\" aggregation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EdgeConv(MessagePassing):\n",
    "    def __init__(self,in_channels, out_channels): # Message Passing Initialization Component\n",
    "        super().__init__(aggr=\"max\") # initialize with aggregation mode of 'add', could also add flow, and node_dim args\n",
    "        self.mlp = Seq(Linear(2 * in_channels, out_channels),\n",
    "                       ReLU(),\n",
    "                       Linear(out_channels, out_channels))\n",
    "\n",
    "\n",
    "    ''' forward contains the propagate step, which'''\n",
    "    def forward(self, x, edge_index):\n",
    "        self.propagate(x, edge_index)\n",
    "    \n",
    "    def message(self, x_i, x_j):\n",
    "        # x_i has shape [E, in_channels]\n",
    "        # x_j has shape [E, in_channels]\n",
    "        return self.mlp(torch.cat([x_i,(x_i-x_j)],dim=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a GNN from a Point Cloud ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import knn_graph\n",
    "\n",
    "class DynamicEdgeConv(EdgeConv):\n",
    "    def __init__(self, in_channels, out_channels, k=6):\n",
    "        super().__init__(in_channels, out_channels)\n",
    "        self.k = k\n",
    "\n",
    "    def forward(self, x, batch=None):\n",
    "        edge_index = knn_graph(x, self.k, batch, loop=False, flow=self.flow)\n",
    "        return super().forward(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 1, 2]), tensor([1, 0, 2, 1]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('gnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e152375ef42f73320e4669c68a4373ab91f49d66122716ccc3f1ce41ec08c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
